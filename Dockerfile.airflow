# Use the official Airflow 3.0.2 image as a base, which uses Python 3.12.
FROM apache/airflow:3.0.2

# Switch to the root user to install system packages
USER root

# Install system dependencies.
# - openjdk-17-jre-headless: Required for Spark to run.
# - wget: To download the Spark distribution.
# - procps, redis-tools, netcat-openbsd: Utilities for health checks and diagnostics.
# - python3.12, python3-pip: Explicitly install to ensure a complete Python environment.
RUN apt-get update -qq && \
    apt-get install -yqq --no-install-recommends \
    openjdk-17-jre-headless \
    wget \
    procps \
    redis-tools \
    netcat-openbsd \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Define Spark version to match the Spark cluster
ARG SPARK_VERSION="3.5.1"
ARG HADOOP_VERSION="3"
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

# Download and install Spark. This is required for the SparkSubmitOperator to submit jobs
# to the cluster in 'client' mode, as it needs the Spark binaries locally.
RUN wget -q "https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz" -O /tmp/spark.tgz && \
    tar -xzf /tmp/spark.tgz -C /opt && \
    mv "/opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}" "${SPARK_HOME}" && \
    rm /tmp/spark.tgz

# Install additional python packages from a requirements file
COPY --chown=airflow:root ./requirements-airflow.txt /requirements-airflow.txt
USER airflow
RUN pip install --no-cache-dir -r /requirements-airflow.txt